{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoCorrect Using N-Gram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE8zv0mgWYfm",
        "outputId": "e78e3595-426c-4ce6-e0f3-89649f12d26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.data.path.append('.')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('en_US.twitter.txt','r') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "BQGHXv9CXIwJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data) # total letters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLfUOEFZXaDA",
        "outputId": "60d020fe-28a8-48b5-e575-643c75c9117a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3335477"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "  sentences = data.split('\\n')\n",
        "  sentences = [s.strip() for s in sentences]\n",
        "  sentences = [s for s in sentences if len(s)>0]\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "ZZ5wSqrnXlEj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "  words = []\n",
        "  for sentence in sentences:\n",
        "    words.append(nltk.word_tokenize(sentence.lower()))\n",
        "  return words"
      ],
      "metadata": {
        "id": "YSn5WS6zYHTw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = split_data(data)\n",
        "len(sentences) # total sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwvn_ZF_YGDg",
        "outputId": "255334be-94a0-46aa-b000-1541ac34746e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47961"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = tokenize(sentences)\n",
        "tokenized_data[0] # tokens for first sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly1-py6laPRw",
        "outputId": "95a05b73-0ab6-45e4-edd0-af50309adadd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how',\n",
              " 'are',\n",
              " 'you',\n",
              " '?',\n",
              " 'btw',\n",
              " 'thanks',\n",
              " 'for',\n",
              " 'the',\n",
              " 'rt',\n",
              " '.',\n",
              " 'you',\n",
              " 'gon',\n",
              " 'na',\n",
              " 'be',\n",
              " 'in',\n",
              " 'dc',\n",
              " 'anytime',\n",
              " 'soon',\n",
              " '?',\n",
              " 'love',\n",
              " 'to',\n",
              " 'see',\n",
              " 'you',\n",
              " '.',\n",
              " 'been',\n",
              " 'way',\n",
              " ',',\n",
              " 'way',\n",
              " 'too',\n",
              " 'long',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(tokenized_data)\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ],
      "metadata": {
        "id": "RByddiIOarp1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(sentences):\n",
        "  word_count = {}\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word not in word_count.keys():\n",
        "        word_count[word] = 1\n",
        "      else:\n",
        "        word_count[word]+=1\n",
        "  return word_count"
      ],
      "metadata": {
        "id": "jqBC__RJbRbT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_frequency(tokenized_sentences,threshold):\n",
        "  closed_vocab = []\n",
        "  word_count = count_words(tokenized_sentences)\n",
        "  for word,freq in word_count.items():\n",
        "    if freq>=threshold:\n",
        "      closed_vocab.append(word)\n",
        "  return closed_vocab"
      ],
      "metadata": {
        "id": "M17pS-ytciI1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_with_unk(tokenized_sentences,vocab,unknown_token='<unk>'):\n",
        "  replaced_sentence = []\n",
        "  for sentence in tokenized_sentences:\n",
        "    modified_sentence = []\n",
        "    for word in sentence:\n",
        "      if word in vocab:\n",
        "        modified_sentence.append(word)\n",
        "      else:\n",
        "        modified_sentence.append(unknown_token)\n",
        "    replaced_sentence.append(modified_sentence)\n",
        "  return replaced_sentence"
      ],
      "metadata": {
        "id": "jqL4p_4IjIAQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(train_data,test_data,threshold):\n",
        "  vocab = get_word_frequency(train_data,threshold)\n",
        "  train_data_replaced = replace_oov_with_unk(train_data,vocab)\n",
        "  test_data_replaced = replace_oov_with_unk(test_data,vocab)\n",
        "  return vocab,train_data_replaced,test_data_replaced"
      ],
      "metadata": {
        "id": "3wSMhDp_kzd2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab,train_data_processed, test_data_processed = preprocess_data(train_data, test_data,2)"
      ],
      "metadata": {
        "id": "3IAuwUhem8JT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R6s14Tgn8li",
        "outputId": "e30f2c47-239b-46d4-8fe6-915ca6540274"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14884"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data,n,start_token='<s>',end_token='<e>'):\n",
        "  n_grams = {}\n",
        "  for sentence in data:\n",
        "    modified_sentence = [start_token] * n + sentence + [end_token]\n",
        "    sent = tuple(modified_sentence)\n",
        "    m = len(modified_sentence) if n==1 else len(modified_sentence)-1\n",
        "    for i in range(m):\n",
        "      n_gram = sent[i:i+n]\n",
        "      if n_gram in n_grams.keys():\n",
        "        n_grams[n_gram]+=1\n",
        "      else:\n",
        "        n_grams[n_gram]=1\n",
        "  return n_grams"
      ],
      "metadata": {
        "id": "h24AlkdyoD2y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(word,previous_n_gram,n_gram_count,nplus1_gram_count,vocab_size,k=1):\n",
        "  prev_n_gram = tuple(previous_n_gram)\n",
        "  nplus1_gram = prev_n_gram + (word,)\n",
        "  \n",
        "  if nplus1_gram in nplus1_gram_count:\n",
        "    num = nplus1_gram_count[nplus1_gram] + k\n",
        "  else:\n",
        "    num = k\n",
        "\n",
        "  if prev_n_gram in n_gram_count:\n",
        "    den = n_gram_count[prev_n_gram] + (k * vocab_size)\n",
        "  else:\n",
        "    den = k * vocab_size  \n",
        "\n",
        "  prob = num/den\n",
        "  return prob"
      ],
      "metadata": {
        "id": "ONVp9k8MuW6b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    \n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    \n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = prob(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "NMQaUzoZ02jN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "    \n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "\n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "    \n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ],
      "metadata": {
        "id": "qXu_2z4d2rOP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, vocabulary)\n",
        "    count_matrix += k # Smoothing\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "metadata": {
        "id": "0I8R5a214c5y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)\n",
        "    \n",
        "    product_pi = 1.0\n",
        "    \n",
        "    for t in range(n, N): \n",
        "        n_gram = sentence[t-n:t]\n",
        "        word = sentence[t]\n",
        "\n",
        "        probability = prob(word,n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1)\n",
        "\n",
        "        product_pi *= 1 / probability\n",
        "\n",
        "    perplexity = product_pi**(1/float(N))\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "o-xaeq3m9Dp7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "\n",
        "perplexity_train1 = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
        "\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_perplexity(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6OQK8jx9m_t",
        "outputId": "6e05c657-58d3-406c-b72b-86f521fa675d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for first train sample: 2.8040\n",
            "Perplexity for test sample: 3.9654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    probabilities = estimate_probabilities(previous_n_gram,n_gram_counts, n_plus1_gram_counts,vocabulary, k=k)\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "\n",
        "    for word, prob in probabilities.items(): \n",
        "        if start_with != None: \n",
        "            if not word.startswith(start_with): \n",
        "                continue \n",
        "        \n",
        "        if prob > max_prob: \n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob"
      ],
      "metadata": {
        "id": "f1ut-LU1AMzc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\\n\")\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKLT5awvAknE",
        "outputId": "8e5ad921-7c3a-4c74-92e1-de6224a60584"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are 'i like',\n",
            "\tand the suggested word is `a` with a probability of 0.2727\n",
            "\n",
            "The previous words are 'i like', the suggestion must start with `c`\n",
            "\tand the suggested word is `cat` with a probability\n"
          ]
        }
      ]
    }
  ]
}